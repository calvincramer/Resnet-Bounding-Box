{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported all successfully!\n"
     ]
    }
   ],
   "source": [
    "# import IPython;IPython.embed()\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import argparse\n",
    "\n",
    "# Import things from actual YOLO folder\n",
    "sys.path.insert(0, '/root/PyTorch-YOLOv3')\n",
    "\n",
    "from models import *\n",
    "from utils.utils import *\n",
    "from utils.datasets import *\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.ticker import NullLocator\n",
    "\n",
    "print(\"Imported all successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters to always use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_folder = '../PyTorch-YOLOv3/data/samples'\n",
    "config_path = '../PyTorch-YOLOv3/config/yolov3.cfg'\n",
    "weights_path = '../PyTorch-YOLOv3/weights/yolov3.weights'\n",
    "class_path = '../PyTorch-YOLOv3/data/coco.names'\n",
    "output_folder = \"OUTPUT_IMAGES\"\n",
    "conf_thres = 0.8\n",
    "nms_thres = 0.4\n",
    "batch_size = 1\n",
    "n_cpu = 8\n",
    "img_size = 416\n",
    "use_cuda = True\n",
    "cuda = torch.cuda.is_available() and use_cuda # Whether the gpu is available\n",
    "os.makedirs(output_folder, exist_ok=True)   \n",
    "model = Darknet(config_path, img_size=img_size) # Set up model\n",
    "model.load_weights(weights_path)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "model.eval() # Set in evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bounding-box colors\n",
    "cmap = plt.get_cmap('tab20b')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def infer_draw_frame(frame):\n",
    "    # TODO: NEED TO CREATE A DATALOADER WITH JUST THIS ONE FRAME, OR COULD SEND FILENAME AS PARAMETER?\n",
    "    dataloader = DataLoader(ImageFolder(image_folder, img_size=img_size), batch_size=batch_size, shuffle=False, num_workers=n_cpu)\n",
    "    classes = load_classes(class_path) # Extracts class labels from file\n",
    "    Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "    imgs = []           # Stores image paths\n",
    "    img_detections = [] # Stores detections for each image index\n",
    "    print (\"\\nPerforming object detection:\")\n",
    "    prev_time = time.time()\n",
    "    for batch_i, (img_paths, input_imgs) in enumerate(dataloader):\n",
    "        # Configure input\n",
    "        input_imgs = Variable(input_imgs.type(Tensor))\n",
    "        # Get detections\n",
    "        with torch.no_grad():\n",
    "            detections = model(input_imgs)\n",
    "            detections = non_max_suppression(detections, 80, conf_thres, nms_thres)\n",
    "        # Log progress\n",
    "        current_time = time.time()\n",
    "        inference_time = datetime.timedelta(seconds=current_time - prev_time)\n",
    "        prev_time = current_time\n",
    "        print ('\\t+ Batch %d, Inference Time: %s' % (batch_i, inference_time))\n",
    "        # Save image and detections\n",
    "        # DONT NEED TO SAVE, JUST DRAW DIRECTLY ON THE FRAME\n",
    "        imgs.extend(img_paths)\n",
    "        img_detections.extend(detections)\n",
    "        \n",
    "    print(\"Done with inferrence\")\n",
    "    #import IPython;IPython.embed()\n",
    "    return THE FRAME WITH INFERRENCE BOXES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "video_file = \"input-video.avi\"\n",
    "output_folder = \"OUTPUT_VIDEO_FRAMES\"\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create video capture object to read video frames\n",
    "vid_read = cv2.VideoCapture(video_file)\n",
    "# Create video writer object\n",
    "fourcc = cv2.VideoWriter_fourcc('M', 'J', 'P', 'G')\n",
    "vid_write = cv2.VideoWriter('OUTPUT_VIDEO.avi', fourcc, 30.0, (1280, 720))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "success, frame = vid_read.read()\n",
    "print(\"Data type of frame: \", type(frame))\n",
    "print(\"Frame size: \", frame.shape, \"\\tin width height channels\")\n",
    "while success:\n",
    "    frame = infer_draw_frame(frame)\n",
    "    vid_write.write(frame)\n",
    "    success, frame = vid_read.read()\n",
    "vid_read.release()\n",
    "vid_write.release()\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
