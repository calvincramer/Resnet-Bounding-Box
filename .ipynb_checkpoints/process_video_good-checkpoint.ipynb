{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported all successfully!\n"
     ]
    }
   ],
   "source": [
    "# import IPython;IPython.embed()\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import argparse\n",
    "\n",
    "# Import things from actual YOLO folder\n",
    "sys.path.insert(0, '/root/PyTorch-YOLOv3')\n",
    "\n",
    "from models import *\n",
    "from utils.utils import *\n",
    "from utils.datasets import *\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib\n",
    "#matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.ticker import NullLocator\n",
    "\n",
    "import cv2\n",
    "from scipy.misc import imresize\n",
    "\n",
    "# Need?\n",
    "%matplotlib inline \n",
    "\n",
    "print(\"Imported all successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters to always use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config_path = \"../PyTorch-YOLOv3/config/yolov3.cfg\"\n",
    "weights_path = \"../PyTorch-YOLOv3/weights/yolov3.weights\"\n",
    "class_path = \"../PyTorch-YOLOv3/data/coco.names\"\n",
    "conf_thres = 0.8\n",
    "nms_thres = 0.4\n",
    "batch_size = 1\n",
    "n_cpu = 8\n",
    "img_size = 416\n",
    "use_cuda = True\n",
    "cuda = torch.cuda.is_available() and use_cuda # Whether the gpu is available\n",
    "model = Darknet(config_path, img_size=img_size) # Set up model\n",
    "model.load_weights(weights_path)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "model.eval() # Set in evaluation mode\n",
    "classes = load_classes(class_path) # Extracts class labels from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bounding-box colors\n",
    "cmap = plt.get_cmap('tab20b')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_frame(frame, detections):\n",
    "    # make sure return is numpy array of same dimensions as input frame\n",
    "    frame = imresize(frame, (720, 1280))\n",
    "\n",
    "    # The amount of padding that was added\n",
    "    pad_x = max(frame.shape[0] - frame.shape[1], 0) * (img_size / max(frame.shape))\n",
    "    pad_y = max(frame.shape[1] - frame.shape[0], 0) * (img_size / max(frame.shape))\n",
    "    # Image height and width after padding is removed\n",
    "    unpad_h = img_size - pad_y\n",
    "    unpad_w = img_size - pad_x\n",
    "    \n",
    "    # Draw bounding boxes and labels of detections\n",
    "    unique_labels = detections[0][:, -1].cpu().unique()\n",
    "    n_cls_preds = len(unique_labels)\n",
    "    bbox_colors = random.sample(colors, n_cls_preds)\n",
    "    for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections[0]:    # data in detection array!\n",
    "        #print ('\\t+ Label: %s, Conf: %.5f' % (classes[int(cls_pred)], cls_conf.item()))\n",
    "        # Rescale coordinates to original dimensions\n",
    "        box_h = ((y2 - y1) / unpad_h) * frame.shape[0]\n",
    "        box_w = ((x2 - x1) / unpad_w) * frame.shape[1]\n",
    "        y1 = ((y1 - pad_y // 2) / unpad_h) * frame.shape[0]\n",
    "        x1 = ((x1 - pad_x // 2) / unpad_w) * frame.shape[1]\n",
    "\n",
    "        color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n",
    "        cv2.rectangle(frame, (x1, y1), (x1 + box_w, y1 + box_h), (255,0,0), 5)\n",
    "        cv2.putText(frame, classes[int(cls_pred)], (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1.25, (255,255,255), 2)\n",
    "        \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def infer_draw_frame(frame):    \n",
    "    # Convert the frame into a tensor\n",
    "    frame_tensor = torch.from_numpy(frame)\n",
    "    frame_tensor = frame_tensor.unsqueeze(0) # Add the batch dimension\n",
    "    frame_tensor = frame_tensor.permute(0, 3, 1, 2) # Swap axes to [b,c,w,h] form\n",
    "    frame_tensor = frame_tensor.float() # Convert byte tensor to float tensor\n",
    "    frame_tensor = frame_tensor / 255   # Normalize data\n",
    "        \n",
    "    Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        detections = model(frame_tensor)\n",
    "        detections = non_max_suppression(detections, 80, conf_thres, nms_thres)\n",
    "        \n",
    "    #import IPython;IPython.embed()\n",
    "        \n",
    "    current_time = time.time()\n",
    "    inference_time = datetime.timedelta(seconds=current_time - start_time)\n",
    "    print (\"\\tInferrence time: \", inference_time)\n",
    "        \n",
    "    final_frame = draw_frame(frame, detections)\n",
    "    return final_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "video_input_filename = \"input-video-short.avi\"\n",
    "video_output_filename = \"OUTPUT_VIDEO.avi\"\n",
    "# Create video capture object to read video frames\n",
    "vid_read = cv2.VideoCapture(video_input_filename)\n",
    "TOTAL_FRAMES = int(vid_read.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(\"Number of frames: \", TOTAL_FRAMES)\n",
    "# Create video writer object\n",
    "fourcc = cv2.VideoWriter_fourcc('M', 'J', 'P', 'G')\n",
    "vid_write = cv2.VideoWriter(video_output_filename, fourcc, 30.0, (1280, 720))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "success, frame = vid_read.read()\n",
    "print(\"Data type of frame: \", type(frame))\n",
    "print(\"Frame size: \", frame.shape, \"\\tin width height channels\")\n",
    "frame_num = 0\n",
    "while success and frame_num < 10:\n",
    "    print(\"Frame \", frame_num, \"/\", TOTAL_FRAMES, \"=\", \"%0.2f\" % (frame_num / TOTAL_FRAMES), end=\"\")\n",
    "    # Resize numpy array to correct size\n",
    "    resized_frame = imresize(frame, (img_size, img_size))\n",
    "    output_frame = infer_draw_frame(resized_frame)    \n",
    "    vid_write.write(output_frame)\n",
    "    # Read next frame from input\n",
    "    success, frame = vid_read.read()\n",
    "    frame_num += 1    \n",
    "    \n",
    "# Clean up    \n",
    "vid_read.release()\n",
    "vid_write.release()\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
