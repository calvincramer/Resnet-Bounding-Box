{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported all successfully!\n"
     ]
    }
   ],
   "source": [
    "# import IPython;IPython.embed()\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import argparse\n",
    "\n",
    "# Import things from actual YOLO folder\n",
    "sys.path.insert(0, '/root/PyTorch-YOLOv3')\n",
    "\n",
    "from models import *\n",
    "from utils.utils import *\n",
    "from utils.datasets import *\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.ticker import NullLocator\n",
    "\n",
    "print(\"Imported all successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_folder = '../PyTorch-YOLOv3/data/samples'\n",
    "config_path = '../PyTorch-YOLOv3/config/yolov3.cfg'\n",
    "weights_path = '../PyTorch-YOLOv3/weights/yolov3.weights'\n",
    "class_path = '../PyTorch-YOLOv3/data/coco.names'\n",
    "output_folder = \"OUTPUT_IMAGES\"\n",
    "conf_thres = 0.8\n",
    "nms_thres = 0.4\n",
    "batch_size = 1\n",
    "n_cpu = 8\n",
    "img_size = 416\n",
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing object detection:\n",
      "Python 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) \n",
      "Type 'copyright', 'credits' or 'license' for more information\n",
      "IPython 6.1.0 -- An enhanced Interactive Python. Type '?' for help.\n",
      "\n",
      "In [1]: \n",
      "\n",
      "In [1]: type(input_imgs)\n",
      "Out[1]: torch.Tensor\n",
      "\n",
      "In [2]: input_imgs.type()\n",
      "Out[2]: 'torch.FloatTensor'\n",
      "\n",
      "In [3]: input_imgs.size\n",
      "Out[3]: <function Tensor.size>\n",
      "\n",
      "In [4]: input_imgs.shape\n",
      "Out[4]: torch.Size([1, 3, 416, 416])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available() and use_cuda # Whether the gpu is available\n",
    "os.makedirs(output_folder, exist_ok=True)   \n",
    "model = Darknet(config_path, img_size=img_size) # Set up model\n",
    "model.load_weights(weights_path)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "model.eval() # Set in evaluation mode\n",
    "dataloader = DataLoader(ImageFolder(image_folder, img_size=img_size), batch_size=batch_size, shuffle=False, num_workers=n_cpu)\n",
    "classes = load_classes(class_path) # Extracts class labels from file\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "imgs = []           # Stores image paths\n",
    "img_detections = [] # Stores detections for each image index\n",
    "print (\"\\nPerforming object detection:\")\n",
    "prev_time = time.time()\n",
    "for batch_i, (img_paths, input_imgs) in enumerate(dataloader):\n",
    "    # Configure input\n",
    "    input_imgs = Variable(input_imgs.type(Tensor))\n",
    "    \n",
    "    import IPython;IPython.embed()\n",
    "    \n",
    "    # Get detections\n",
    "    with torch.no_grad():\n",
    "        detections = model(input_imgs)\n",
    "        detections = non_max_suppression(detections, 80, conf_thres, nms_thres)\n",
    "    # Log progress\n",
    "    current_time = time.time()\n",
    "    inference_time = datetime.timedelta(seconds=current_time - prev_time)\n",
    "    prev_time = current_time\n",
    "    print ('\\t+ Batch %d, Inference Time: %s' % (batch_i, inference_time))\n",
    "    # Save image and detections\n",
    "    imgs.extend(img_paths)\n",
    "    img_detections.extend(detections)\n",
    "print(\"Done with inferrence\")\n",
    "#import IPython;IPython.embed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bounding-box colors\n",
    "cmap = plt.get_cmap('tab20b')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n",
    "\n",
    "print ('\\nSaving images:')\n",
    "# Iterate through images and save plot of detections\n",
    "for img_i, (path, detections) in enumerate(zip(imgs, img_detections)):\n",
    "    print (\"(%d) Image: '%s'\" % (img_i, path))\n",
    "\n",
    "    # Create plot\n",
    "    img = np.array(Image.open(path))\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # The amount of padding that was added\n",
    "    pad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))\n",
    "    pad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))\n",
    "    # Image height and width after padding is removed\n",
    "    unpad_h = img_size - pad_y\n",
    "    unpad_w = img_size - pad_x\n",
    "\n",
    "    # Draw bounding boxes and labels of detections\n",
    "    if detections is not None:\n",
    "        unique_labels = detections[:, -1].cpu().unique()\n",
    "        n_cls_preds = len(unique_labels)\n",
    "        bbox_colors = random.sample(colors, n_cls_preds)\n",
    "        for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:    # data in detection array!\n",
    "            print ('\\t+ Label: %s, Conf: %.5f' % (classes[int(cls_pred)], cls_conf.item()))\n",
    "\n",
    "            # Rescale coordinates to original dimensions\n",
    "            box_h = ((y2 - y1) / unpad_h) * img.shape[0]\n",
    "            box_w = ((x2 - x1) / unpad_w) * img.shape[1]\n",
    "            y1 = ((y1 - pad_y // 2) / unpad_h) * img.shape[0]\n",
    "            x1 = ((x1 - pad_x // 2) / unpad_w) * img.shape[1]\n",
    "\n",
    "            color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n",
    "            # Create a Rectangle patch\n",
    "            bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2, edgecolor=color, facecolor='none')\n",
    "            # Add the bbox to the plot\n",
    "            ax.add_patch(bbox)\n",
    "            # Add label\n",
    "            plt.text(x1, y1, s=classes[int(cls_pred)], color='white', verticalalignment='top', bbox={'color': color, 'pad': 0})\n",
    "\n",
    "    # Save generated image with detections\n",
    "    plt.axis('off')\n",
    "    plt.gca().xaxis.set_major_locator(NullLocator())\n",
    "    plt.gca().yaxis.set_major_locator(NullLocator())\n",
    "   \n",
    "    plt.savefig(output_folder + \"/%d.png\" % (img_i), dpi=1000, bbox_inches='tight', pad_inches=0.0)\n",
    "    \n",
    "    # Instead of saving the file, convert to numpy array\n",
    "    fig.canvas.draw()\n",
    "    pic_numpy = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "    pic_numpy = pic_numpy.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse through the video frame by frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "video_file = \"input-video.avi\"\n",
    "output_folder = \"OUTPUT_VIDEO_FRAMES\"\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create video capture object to read video frames\n",
    "vid_read = cv2.VideoCapture(video_file)\n",
    "# Create video writer object\n",
    "fourcc = cv2.VideoWriter_fourcc('M', 'J', 'P', 'G')\n",
    "vid_write = cv2.VideoWriter('OUTPUT_VIDEO.avi', fourcc, 30.0, (1280, 720))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "success, frame = vid_read.read()\n",
    "print(\"Data type of frame: \", type(frame))\n",
    "print(\"Frame size: \", frame.shape, \"\\tin width height channels\")\n",
    "frame_number = 0\n",
    "while success:\n",
    "    # Save individual frames\n",
    "    #cv2.imwrite(output_folder + \"/frame%d.jpg\" % frame_number, image)     # save frame as JPEG file     \n",
    "    \n",
    "    # Process one frame ...\n",
    "    frame = cv2.flip(frame,0)\n",
    "    # write the flipped frame\n",
    "    vid_write.write(frame)\n",
    "    \n",
    "     \n",
    "    \n",
    "    success, frame = vid_read.read()\n",
    "    #frame_number += 1\n",
    "    \n",
    "vid_read.release()\n",
    "vid_write.release()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
